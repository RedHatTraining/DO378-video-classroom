WEBVTT

00:00:00.000 --> 00:00:01.035
Welcome back!

00:00:01.035 --> 00:00:03.570
Now, we are going to have

00:00:03.570 --> 00:00:06.750
a large number of microservices
running at some point

00:00:06.750 --> 00:00:11.385
and we want to introduce some
insight in order into that.

00:00:11.385 --> 00:00:13.530
Without any additional tooling,

00:00:13.530 --> 00:00:18.495
what your microservices
landscape looks like?

00:00:18.495 --> 00:00:21.615
It's basically just the
diagram we see here.

00:00:21.615 --> 00:00:24.820
There's a larger number
of potential connections,

00:00:24.820 --> 00:00:28.910
but without any clear
indication who's calling what,

00:00:28.910 --> 00:00:32.060
when to perform whatever action.

00:00:32.060 --> 00:00:35.720
So what we ideally want is something that

00:00:35.720 --> 00:00:39.455
tells us that when user
invokes Service 1,

00:00:39.455 --> 00:00:42.200
that one in turn invokes Service

00:00:42.200 --> 00:00:44.765
2 which cascades the Service 3

00:00:44.765 --> 00:00:48.125
and then Services 9 and
10 also getting invoked.

00:00:48.125 --> 00:00:48.815
Right!

00:00:48.815 --> 00:00:50.930
But if possible, actually,

00:00:50.930 --> 00:00:55.955
it would be even better if we could
be in the position which allows us to

00:00:55.955 --> 00:01:01.940
reassemble a service call graph
to see which services,

00:01:01.940 --> 00:01:05.750
for example, are called
directly from Service A.

00:01:05.750 --> 00:01:11.930
And then which services refer
to other services in what way?

00:01:11.930 --> 00:01:14.810
Either in sequence, in
parallel, and so on.

00:01:14.810 --> 00:01:18.425
It's actually possible to
do that by implementing

00:01:18.425 --> 00:01:21.290
the or by using an implementation of

00:01:21.290 --> 00:01:24.814
the OpenTracing API
with your application.

00:01:24.814 --> 00:01:27.320
Distributed tracing,

00:01:27.320 --> 00:01:32.929
actually is specifically designed to
allow developers and administrators

00:01:32.929 --> 00:01:35.870
to trace calls and visualize

00:01:35.870 --> 00:01:40.925
them when they are traversing
a number of microservices.

00:01:40.925 --> 00:01:46.505
So it's very important to understand
the sequence of those calls, of course,

00:01:46.505 --> 00:01:49.040
because every invocation changes

00:01:49.040 --> 00:01:53.360
the payload that is manipulated by the
application in some way or another.

00:01:53.360 --> 00:01:55.940
And of course, not just the sequence,

00:01:55.940 --> 00:01:58.670
but also the duration of those calls

00:01:58.670 --> 00:02:01.970
and the manner or the parallelism
level they are made in.

00:02:01.970 --> 00:02:04.820
So if 2 invocations are
performed in sequence,

00:02:04.820 --> 00:02:07.310
then the latency associated with each of

00:02:07.310 --> 00:02:11.120
those services is added
on top of the response time.

00:02:11.120 --> 00:02:13.820
Whereas if we are invoking
services in parallel,

00:02:13.820 --> 00:02:16.985
then obviously the longer one
is the amount of time

00:02:16.985 --> 00:02:20.645
that adds up to the total response.

00:02:20.645 --> 00:02:26.030
It's much easier to identify
the culprit of slow responses,

00:02:26.030 --> 00:02:30.680
of potential errors in
responses and so on,

00:02:30.680 --> 00:02:33.995
if you have distributed
tracing implemented.

00:02:33.995 --> 00:02:40.970
The way distributed tracing is
defined is, it introduces 2 terms,

00:02:40.970 --> 00:02:43.550
that of a span and that of a trace.

00:02:43.550 --> 00:02:45.350
A trace is basically a

00:02:45.350 --> 00:02:51.725
total execution path of a single
service invocation in the mesh.

00:02:51.725 --> 00:02:54.500
So when a user invokes the service,

00:02:54.500 --> 00:02:56.435
it starts a trace.

00:02:56.435 --> 00:02:59.570
And then whichever other microservices are

00:02:59.570 --> 00:03:03.020
involved in the resolution of
the initial user's request,

00:03:03.020 --> 00:03:06.035
become a part of that same trace.

00:03:06.035 --> 00:03:11.615
Each path of a request
wherever it happens,

00:03:11.615 --> 00:03:15.980
downstream and back to the
originator of that request,

00:03:15.980 --> 00:03:19.310
which obviously receives a
response then is called a span.

00:03:19.310 --> 00:03:23.165
So traces consists of multiple spans

00:03:23.165 --> 00:03:26.570
but also spans can contain other spans.

00:03:26.570 --> 00:03:30.140
Let's have a look at that. With the
call graph that we are seeing here,

00:03:30.140 --> 00:03:35.435
so Service A invokes
Services B and E in turn.

00:03:35.435 --> 00:03:44.040
And then Service B forwards that
work to Services C and D as well.

00:03:44.170 --> 00:03:48.380
The total trace would
look something like this.

00:03:48.380 --> 00:03:55.370
So if we have the time on the x scale
and the trace levels on the y scale,

00:03:55.370 --> 00:04:00.320
the first invocation is that of Span A.

00:04:00.320 --> 00:04:03.080
So when the user makes
a request to Service A,

00:04:03.080 --> 00:04:05.720
it starts a span that

00:04:05.720 --> 00:04:10.730
actually incorporates all the
other requests in this trace.

00:04:10.730 --> 00:04:17.720
Service A then invokes
Service B and starts Span B,

00:04:17.720 --> 00:04:21.350
which is the total amount of
time the Service B needs for

00:04:21.350 --> 00:04:26.045
its own processing before it
returns a response to Service A.

00:04:26.045 --> 00:04:29.825
And then Span B contains 2 other span,

00:04:29.825 --> 00:04:33.230
Span C and Span D,

00:04:33.230 --> 00:04:37.010
where Service B is invoking
Server C waiting for

00:04:37.010 --> 00:04:40.760
its response and then in turn invoking

00:04:40.760 --> 00:04:44.120
Service D and waiting for
its response before it's

00:04:44.120 --> 00:04:48.095
able to return the response
back to Service A.

00:04:48.095 --> 00:04:52.460
So in the relationship 
of these spans,

00:04:52.460 --> 00:04:57.515
Span A contains Span B and C and D,

00:04:57.515 --> 00:05:00.935
or a Span B contains Span C and D.

00:05:00.935 --> 00:05:04.040
And then of course, the last
request is to service E,

00:05:04.040 --> 00:05:08.600
which returns and terminating Span A,

00:05:08.600 --> 00:05:11.960
also enables Span A to terminate.

00:05:11.960 --> 00:05:15.140
One of the very popular
tracing platforms which

00:05:15.140 --> 00:05:19.940
uses these approaches
is called Jaeger.

00:05:19.940 --> 00:05:25.700
Jaeger allows developers to configure
services to enrich the requests and

00:05:25.700 --> 00:05:28.774
responses, so as to allow

00:05:28.774 --> 00:05:33.184
gathering of the runtime statistics
about their performance.

00:05:33.184 --> 00:05:36.305
Jaeger is made out of several components.

00:05:36.305 --> 00:05:38.930
They all work together to collect,

00:05:38.930 --> 00:05:41.570
store, and display tracing data.

00:05:41.570 --> 00:05:45.930
Now, the component that

00:05:45.930 --> 00:05:51.194
you will probably be working with
the most is the Jaeger Client.

00:05:51.194 --> 00:05:53.935
The Jaeger Client is basically

00:05:53.935 --> 00:05:58.705
the language specific
implementation because Jaeger

00:05:58.705 --> 00:06:01.690
has support for many different languages.

00:06:01.690 --> 00:06:06.580
The client is the language specific
implementation of the OpenTracing API.

00:06:06.580 --> 00:06:11.635
It is the client that adds the
additional headers to request

00:06:11.635 --> 00:06:18.160
that allow us to establish that
span x is part of a trace 1,

00:06:18.160 --> 00:06:22.060
2, 3 and is contained by span y.

00:06:22.060 --> 00:06:27.385
These additional headers
have to be collected though,

00:06:27.385 --> 00:06:31.825
by some sort of a data collector

00:06:31.825 --> 00:06:37.114
and is the client actually that also
forwards the information about what span

00:06:37.114 --> 00:06:42.785
IDs it has assigned to a certain request
and what traces they are a part of?

00:06:42.785 --> 00:06:45.665
It reports that to Jaeger Agent.

00:06:45.665 --> 00:06:48.125
Jaeger Agent is a network service that

00:06:48.125 --> 00:06:51.185
listens for information
about span data,

00:06:51.185 --> 00:06:54.905
which is generally sent
using UDP protocol.

00:06:54.905 --> 00:06:58.520
And then batches the data it receives from

00:06:58.520 --> 00:07:03.305
the client and sends that
to the Jaeger Collector.

00:07:03.305 --> 00:07:06.440
Collector receives runtime statistics from

00:07:06.440 --> 00:07:11.675
multiple agents and places them
into internal queue for processing,

00:07:11.675 --> 00:07:15.980
which in turn ends up in the storage.

00:07:15.980 --> 00:07:20.105
So the Jaeger collector requires a
persistent storage backend otherwise,

00:07:20.105 --> 00:07:22.910
obviously all the tracing
data will be lost,

00:07:22.910 --> 00:07:25.550
when we restart the collector.

00:07:25.550 --> 00:07:28.430
The Query is then a service that

00:07:28.430 --> 00:07:32.480
retrieves runtime statistics
from storage on request

00:07:32.480 --> 00:07:35.690
and the Jaeger Console
is a console that can

00:07:35.690 --> 00:07:42.840
visualize queries about tracing data
that are stored in the Jaeger collector.

00:07:43.000 --> 00:07:45.200
In Quarkus applications,

00:07:45.200 --> 00:07:50.915
you can enable tracing by adding
quarkus-smallrye-opentracing dependency,

00:07:50.915 --> 00:07:57.785
either using Maven add extension or
adding it manually in the pom.xml file.

00:07:57.785 --> 00:08:03.320
Then you enable tracing related properties
in the application.properties file,

00:08:03.320 --> 00:08:07.415
There are a number of 
quarkus.jaeger properties.

00:08:07.415 --> 00:08:09.590
Most important one is to name

00:08:09.590 --> 00:08:14.375
your application uniquely so that
you can distinguish it from others.

00:08:14.375 --> 00:08:20.554
And then choose a sampler type and
specific sample, specific parameters.

00:08:20.554 --> 00:08:22.865
There are several sampler types,

00:08:22.865 --> 00:08:27.815
like const, which collects all
the data from all the requests.

00:08:27.815 --> 00:08:32.485
Probabilistic, which uses
random sampling, rate limited,

00:08:32.485 --> 00:08:38.590
which collects samples at a specific
configurable rate of samples per second.

00:08:38.590 --> 00:08:44.125
And remote, which means that
sampling is controlled from Jaeger

00:08:44.125 --> 00:08:45.130
backend.

00:08:45.130 --> 00:08:47.020
You should be very careful with

00:08:47.020 --> 00:08:51.310
the constant sampler because it
generates immense amounts of data

00:08:51.310 --> 00:08:54.010
and shouldn't be using production whenever

00:08:54.010 --> 00:08:58.060
there is at least some significant
amount of traffic involved.

00:08:58.060 --> 00:09:02.635
Now, depending on the sampler type,

00:09:02.635 --> 00:09:06.190
sampler-param might have
different meanings,

00:09:06.190 --> 00:09:07.945
but for the const sampler,

00:09:07.945 --> 00:09:14.275
it is just percentage of requests
that spans should be collected for.

00:09:14.275 --> 00:09:17.425
It should be a value between 0 and 1,

00:09:17.425 --> 00:09:22.480
where 0 means no sample collection and
1 means collect all the samples.

00:09:22.480 --> 00:09:25.630
The next property is the endpoint.

00:09:25.630 --> 00:09:29.290
The endpoint is basically
the Jaeger agent or

00:09:29.290 --> 00:09:34.330
the collector that the sample
information should be sent to.

00:09:34.330 --> 00:09:38.140
And then we have the propagation level,

00:09:38.140 --> 00:09:43.435
which is actually the depth to which

00:09:43.435 --> 00:09:45.940
headers that are added by

00:09:45.940 --> 00:09:51.430
the Jaeger client will be
reported back to the collector.

00:09:51.430 --> 00:09:57.520
b3 means that this is

00:09:57.520 --> 00:09:59.530
the third level of headers and it's

00:09:59.530 --> 00:10:03.490
required to allow us not
just to attribute spans,

00:10:03.490 --> 00:10:09.970
to a trace, but also to reconstruct the
parent-child relationship in terms of

00:10:09.970 --> 00:10:13.480
which span contains other spans

00:10:13.480 --> 00:10:17.155
and then what spans do these
spans further contain?

00:10:17.155 --> 00:10:21.219
If you want to log the spans that
were reported in the application,

00:10:21.219 --> 00:10:25.525
you can also turn reports or 
log spans property to true.

00:10:25.525 --> 00:10:29.020
So it's sufficient to just
add the quarkus-smallrye-

00:10:29.020 --> 00:10:30.700
opentracing extension to

00:10:30.700 --> 00:10:33.730
your project and configure
application.properties if all

00:10:33.730 --> 00:10:38.500
you want is collect samples
related to REST invocations.

00:10:38.500 --> 00:10:39.195
So,

00:10:39.195 --> 00:10:45.410
anything that your applications
received from external clients,

00:10:45.410 --> 00:10:49.279
starts to trace any subsequent
further invocations

00:10:49.279 --> 00:10:51.440
of other microservices,

00:10:51.440 --> 00:10:55.160
then create spans until the
initial request is resolved.

00:10:55.160 --> 00:10:59.990
So these 2 steps are enough
to enable this kind of tracing.

00:10:59.990 --> 00:11:02.165
However, if you want to trace,

00:11:02.165 --> 00:11:07.670
for example, the internals
of method invocations,

00:11:07.670 --> 00:11:12.905
like, for example, what methods has

00:11:12.905 --> 00:11:16.760
a REST endpoint method
in your service class

00:11:16.760 --> 00:11:20.990
invoked in the data object
access class for example.

00:11:20.990 --> 00:11:25.040
Then in that case, you would need
to annotate select classes and

00:11:25.040 --> 00:11:29.600
methods in your services
with the @Traced annotation.

00:11:29.600 --> 00:11:32.720
And that will also 
create additional data

00:11:32.720 --> 00:11:36.260
that can be reported
back to the Jaeger collector.

00:11:36.260 --> 00:11:39.740
You can view the traces and spans

00:11:39.740 --> 00:11:43.910
that had been generated by our
application using the Jaeger web console,

00:11:43.910 --> 00:11:49.340
probably the easiest way of running
it is by running it in a container.

00:11:49.340 --> 00:11:55.670
There is an image called all-in-one
available from quay.io/

00:11:55.670 --> 00:11:58.250
jaegertracing that actually has

00:11:58.250 --> 00:12:02.455
all the components that we need
for a minimal Jaeger deployment.

00:12:02.455 --> 00:12:04.445
So including the collector,

00:12:04.445 --> 00:12:07.640
including the query and
of course the console,

00:12:07.640 --> 00:12:10.520
they are all present in that one image.

00:12:10.520 --> 00:12:14.090
All we need to do is simply
expose the ports that Jaeger

00:12:14.090 --> 00:12:17.959
needs to collaborate with the client

00:12:17.959 --> 00:12:24.170
and then open the console on ports 16686,

00:12:24.170 --> 00:12:26.840
which looks a little bit like this.

00:12:26.840 --> 00:12:29.480
So in this console, on the left-hand side,

00:12:29.480 --> 00:12:33.290
we have the trace query section where

00:12:33.290 --> 00:12:38.405
we can choose what service we
want to see the traces from,

00:12:38.405 --> 00:12:41.255
what operations we want
to see the traces from?

00:12:41.255 --> 00:12:43.880
If there are any tags, you
can search by the tag.

00:12:43.880 --> 00:12:51.080
You can select how much, whatever
time window you want to review.

00:12:51.080 --> 00:12:52.430
And then of course,

00:12:52.430 --> 00:12:55.610
other properties such as
minimum duration of a request,

00:12:55.610 --> 00:13:01.130
maximum duration of the request
and request the result limiter.

00:13:01.130 --> 00:13:03.530
Once you have selected your parameters,

00:13:03.530 --> 00:13:08.315
you can click Find Traces and you will
get a trace graph here showing you

00:13:08.315 --> 00:13:10.715
how many requests have been traced

00:13:10.715 --> 00:13:14.585
throughout or over the time
window that you have selected.

00:13:14.585 --> 00:13:18.380
Then below that graph you will see

00:13:18.380 --> 00:13:20.975
how many traces have been or

00:13:20.975 --> 00:13:23.855
you will see individual traces
that have been collected.

00:13:23.855 --> 00:13:27.320
Then once you click one of
the traces in that list,

00:13:27.320 --> 00:13:31.955
you will be able to review the
exact path of the request,

00:13:31.955 --> 00:13:35.105
looking at the spans that it created

00:13:35.105 --> 00:13:39.500
and then of course, the sub
spans that it contains.

00:13:39.500 --> 00:13:41.335
Let's move on to the exercise where

00:13:41.335 --> 00:13:45.000
we add tracing capabilities
to our application.

